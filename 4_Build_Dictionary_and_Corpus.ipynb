{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "import multiprocessing\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'google_news'\n",
    "documents_file = os.listdir(dirname)\n",
    "\n",
    "def get_stoplist():\n",
    "    stop = []\n",
    "    for line in open('stopwords.txt','rU'):\n",
    "        stop += line.replace(',','').split()\n",
    "    return stop\n",
    "\n",
    "stopwords = get_stoplist()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for fname in documents_file:\n",
    "    f = open(os.path.join(dirname,fname),'rU')\n",
    "    content = f.read().decode('utf-8').lower()\n",
    "    words_split = re.findall(r\"[\\w']+|[.,!?;]\",content) # memotong kalimat berdasarkan punctuation\n",
    "    \n",
    "    phrases = gensim.models.phrases.Phrases(words_split,min_count=20, threshold=100)\n",
    "    bigram = gensim.models.phrases.Phraser(phrases)\n",
    "    trigram = gensim.models.phrases.Phrases(bigram[words_split],min_count=20, threshold=100)\n",
    "\n",
    "\n",
    "    for idx in range(len(words_split)):\n",
    "        for token in bigram[words_split[idx]]:\n",
    "            if '_' in token:\n",
    "                #Token is a bigram, add to document.\n",
    "                words_split[idx] += token\n",
    "\n",
    "    for idx in range(len(words_split)):\n",
    "        for token in trigram[words_split[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a trigram, add to document.\n",
    "                words_split[idx] += token\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    for token in words_split:\n",
    "        if re.search('[a-zA-Z]', token): # filter hanya huruf saja\n",
    "            if token not in stopwords:\n",
    "                filtered_tokens.append(token)\n",
    "    \n",
    "    title = fname.replace('.txt','')\n",
    "    documents.append(filtered_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Jumlah Document : 297 '\n"
     ]
    }
   ],
   "source": [
    "pprint('Jumlah Document : %d ' % len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'sheila', u'marcia', u'dijenguk', u'anaknya', u'rumah', u'sakit', u'wowkeren', u'sheila', u'marcia', u'mengalami', u'musibah', u'kecelakaan', u'mobil', u'pagi', u'sheila', u'teman', u'wanitanya', u'melodya', u'vanesha', u'mengalami', u'luka', u'serius', u'akibat', u'kecelakaan', u'sheila', u'dirawat', u'rumah', u'sakit', u'medistra', u'jakarta', u'selatan', u'foto', u'beredar', u'instagram', u'sheila', u'dijenguk', u'buah', u'hatinya', u'leticia', u'charlotte', u'nathaniel', u'precious', u'brianna', u'yael', u'mirano', u'sheila', u'nampak', u'berkumpul', u'ketiga', u'anaknya', u'ranjang', u'rumah', u'sakit', u'diajak', u'berfoto', u'wajah', u'sheila', u'bengkak', u'diperban', u'pelipis', u'hidung', u'penampakan', u'terbaru', u'bintang', u'sinetron', u'berkah', u'cinta', u'mengundang', u'simpati', u'netter', u'netter', u'mengaku', u'kasihan', u'wajah', u'bengkak', u'sheila', u'yawla', u'bonyok2', u'gono', u'kasihan', u'netter', u'aduh', u'kasinnya', u'gws', u'celetuk', u'netter', u'patah', u'ya', u'hidungnya', u'yaampun', u'gws', u'yaa', u'seru', u'bengkak', u'lukanya', u'netter', u'smpe', u'bengkak', u'gtu', u'netter']\n"
     ]
    }
   ],
   "source": [
    "print(documents[2][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.save('assets/dictionary_today.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(9511 unique tokens: [u'gadisnya', u'lelaki', u'diinvestigasi', u'sambung', u'pelakunya']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index word for 'gadisnya' : 3514\n"
     ]
    }
   ],
   "source": [
    "word = 'gadisnya'\n",
    "print(\"index word for '%s' : %d\" % (word,dictionary.token2id['gadisnya']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('assets/corpus_today.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 3), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (19, 2), (20, 2), (21, 1), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 3), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 4), (47, 1), (48, 1), (49, 2), (50, 1), (51, 6), (52, 1), (53, 3), (54, 1), (55, 1), (56, 1), (57, 2), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 2), (68, 2), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 2), (76, 1), (77, 1), (78, 1), (79, 1)]\n"
     ]
    }
   ],
   "source": [
    "for vector in corpus[:1]:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
